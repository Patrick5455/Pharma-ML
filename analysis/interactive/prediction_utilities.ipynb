{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# predefined modules\n",
    "# from prediction_utilities import preprocess\n",
    "# from prediction_utilities import preprocess\n",
    "\n",
    "\n",
    "#preprocessing libraries \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_vars(data, show_plot=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    This functions checks for columns with outlers using the IQR method\n",
    "    \n",
    "    It accespts as argmuent a dataset. \n",
    "    show_plot can be set to True to output pairplots of outlier columns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    outliers = [] \n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    num_data = data.select_dtypes(include='number')\n",
    "    result = dict ((((num_data < (Q1 - 1.5 * IQR)) | (num_data > (Q3 + 1.5 * IQR)))==True).any())\n",
    "    for k,v in result.items():\n",
    "        if v == True: \n",
    "            outliers.append(k)\n",
    "    if show_plot:\n",
    "        pair_plot = sns.pairplot(data[outliers]);\n",
    "        print(f'{result},\\n\\n Visualization of outlier columns')\n",
    "        return pair_plot\n",
    "    else:\n",
    "        return data[outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, to_drop=[], save_path='', obj_name='prcsd_data.pkl'):\n",
    "    \n",
    "    \"\"\"\n",
    "    The preprocess function takes as primary argument the data \n",
    "    and peform the following stepwise transformations to it:\n",
    "    \n",
    "    1. impute missing val\n",
    "    ues of numerical and categorical columns \n",
    "    using median and constant values respectively\n",
    "    \n",
    "    2. scales dataset using the RobustScaler (robust to outlier values present in this dataset)\n",
    "    \n",
    "    3. Encodes categorical values to numerical values\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = data.columns.to_list()\n",
    "    \n",
    "    # split data to numeric vs categorical\n",
    "    numeric_features = data.select_dtypes(include=[\n",
    "        'int64', 'float64']).columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(to_drop) > 0:\n",
    "        data = data.drop(to_drop, axis=1)\n",
    "        categorical_features = data.select_dtypes(include=[\n",
    "        'object']).columns\n",
    "        #print(categorical_features) \n",
    "    else: \n",
    "        categorical_features = data.select_dtypes(include=[\n",
    "        'object']).columns\n",
    "        \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', missing_values=np.nan)),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor) ])\n",
    "    \n",
    "    for col in to_drop:\n",
    "        columns.remove(col) \n",
    "    \n",
    "    trans_data = my_pipeline.fit_transform(data)\n",
    "    \n",
    "    if save_path:\n",
    "        pickle_out = open(save_path+obj_name, 'wb')\n",
    "        pickle.dump(trans_data, pickle_out)\n",
    "        pickle_out.close()\n",
    "    \n",
    "    return pd.DataFrame(trans_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipelines(preprocessor,model_algos=[]):\n",
    "    for algo in model_algos:\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', algo)])\n",
    "        pipe.fit(X_train, y_train)   \n",
    "        print(classifier)\n",
    "        print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_datatype(data, dtype, col_list, date_col_name='Date'):\n",
    "    \"\"\"\n",
    "    This converts specified columns names of a data to the specified data type\n",
    "    It also uses the name od the date column of the dataset to know the data columns\n",
    "    ans convert to datetime object\n",
    "    \"\"\"\n",
    "    for i in col_list:\n",
    "        if i != date_col_name:\n",
    "            data = data.astype({i:dtype})\n",
    "        else:\n",
    "            data[date_col_name] = pd.to_datetime(data.Date)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_load_model (action, model=None, model_name='new_model.pickle',\n",
    "                          path = ''):\n",
    "    \n",
    "    if action == \"wb\":\n",
    "        pickle_out = open(path+model_name,action)\n",
    "        pickle.dump(model, pickle_out)\n",
    "        pickle_out.close() \n",
    "    \n",
    "    elif action==\"rb\":\n",
    "        pickle_in = open(path+model_name,action)\n",
    "        model=pickle.load(pickle_in)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(data, num_type='median'):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Fill categorical clumns containing missing vlaues with mode\n",
    "    \n",
    "    Fill numerical  clumns containing missing vlaues with specified type\n",
    "    \n",
    "    median is default\n",
    "    \n",
    "    \"\"\"\n",
    "    cat = categorical_features = data.select_dtypes(include=[\n",
    "        'object']).columns\n",
    "    \n",
    "    num = numeric_features = data.select_dtypes(include=[\n",
    "        'int64', 'float64']).columns\n",
    "    \n",
    "    for col in cat:\n",
    "        data[col].fillna(data[col].mode()[0], inplace=True) \n",
    "        \n",
    "\n",
    "    if num_type == \"median\":\n",
    "        \n",
    "        for col in num:\n",
    "            data[col].fillna(data[col].median(), inplace=True)\n",
    "    \n",
    "    if num_type == \"mean\":\n",
    "        \n",
    "        for col in num:\n",
    "            data[col].fillna(data[col].mean(), inplace=True)\n",
    "            \n",
    "    if num_type == \"mode\":\n",
    "        \n",
    "        for col in num:\n",
    "            data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(data, date_col_name='Date', season=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts various date types from a date column\n",
    "    \n",
    "    If season is specified, adds column for season\n",
    "    \"\"\"\n",
    "    \n",
    "    data[\"Month\"] = data[date_col_name].dt.month\n",
    "    data[\"Quarter\"] = data[date_col_name].dt.quarter\n",
    "    data[\"Year\"] = data[date_col_name].dt.year\n",
    "    data[\"Day\"] = data[date_col_name].dt.day\n",
    "    data[\"Week\"] = data[date_col_name].dt.week\n",
    "    \n",
    "    if season:\n",
    "        data[\"Season\"] = np.where(data[\"Month\"].isin([3,4,5]),\"Spring\",\n",
    "        np.where(data[\"Month\"].isin([6,7,8]),\"Summer\",np.where(data[\"Month\"].isin\n",
    "        ([9,10,11]),\"Fall\",np.where(data[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pharma_project",
   "language": "python",
   "name": "venv_pharma_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
